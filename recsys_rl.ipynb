{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBbEvG1aixZo"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import ceil\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ranger import Ranger\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as td\n",
    "import torch.optim as to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7YiOW9TixZs"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device= 'cpu'\n",
    "\n",
    "# The directory to store the data\n",
    "data_dir = \"data\"\n",
    "train_rating = \"ml-1m.train.rating\"\n",
    "test_negative = \"ml-1m.test.negative\"\n",
    "\n",
    "session_length = 20\n",
    "train_negative_samples = 4\n",
    "test_negative_samples = 99\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128 # 32\n",
    "N = 10 # memory size for state_repr\n",
    "\n",
    "# Training config\n",
    "batch_size = 16 # 512\n",
    "top_k=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BExS1c1zixZv"
   },
   "source": [
    "## Data\n",
    "\n",
    "\n",
    "Use Movielens 1M data from the https://github.com/hexiangnan/neural_collaborative_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "qabQkULCixZv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip loading ml-1m.train.rating\n",
      "Skip loading ml-1m.test.negative\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "    \n",
    "for file_name in [train_rating, test_negative]:\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"Skip loading \" + file_name)\n",
    "        continue\n",
    "    with open(file_path, \"wb\") as tf:\n",
    "        print(\"Load \" + file_name)\n",
    "        r = requests.get(\"https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/\" + file_name)\n",
    "        tf.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "lDR1YFTbixZx"
   },
   "outputs": [],
   "source": [
    "def preprocess_train():\n",
    "    train_data = pd.read_csv(os.path.join(data_dir, train_rating), sep='\\t', header=None, \n",
    "                             names=['user', 'item', 'rating'], usecols=[0, 1, 2], \n",
    "                             dtype={0: np.int32, 1: np.int32, 2: np.int8})\n",
    "    \n",
    "    train_data = train_data[train_data['rating'] > 3][['user', 'item']]\n",
    "    user_num = train_data['user'].max() + 1\n",
    "    item_num = train_data['item'].max() + 1\n",
    "\n",
    "    mat = defaultdict(int)\n",
    "    train_data = train_data.values.tolist()\n",
    "    for user, item in train_data:\n",
    "        mat[user, item] = 1.0\n",
    "        \n",
    "    # Convert ratings as a dok matrix\n",
    "    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "    dict.update(train_mat, mat)\n",
    "    \n",
    "    return train_data, train_mat, user_num, item_num\n",
    "\n",
    "train_data, train_mat, user_num, item_num = preprocess_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "BGqqEVoRixZ0"
   },
   "outputs": [],
   "source": [
    "def preprocess_test():\n",
    "    test_data = []\n",
    "    with open(os.path.join(data_dir, test_negative)) as tnf:\n",
    "        for line in tnf:\n",
    "            parts = line.split('\\t')\n",
    "            assert len(parts) == test_negative_samples + 1\n",
    "            user, positive = eval(parts[0])\n",
    "            test_data.append([user, positive])\n",
    "            \n",
    "            for negative in parts[1:]:\n",
    "                test_data.append([user, int(negative)])\n",
    "\n",
    "    return test_data\n",
    "\n",
    "valid_data = preprocess_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "RUGNP7u0ixZ3"
   },
   "outputs": [],
   "source": [
    "class MLDataset(td.Dataset):\n",
    "    \n",
    "    def __init__(self, positive_data, item_num, positive_mat, negative_samples=0):\n",
    "        super(MLDataset, self).__init__()\n",
    "        self.positive_data = positive_data\n",
    "        self.item_num = item_num\n",
    "        self.positive_mat = positive_mat\n",
    "        self.negative_samples = negative_samples\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        print(\"Resetting dataset\")\n",
    "        if self.negative_samples > 0:\n",
    "            negative_data = self.sample_negatives()\n",
    "            data = self.positive_data + negative_data\n",
    "            labels = [1] * len(self.positive_data) + [0] * len(negative_data)\n",
    "        else:\n",
    "            data = self.positive_data\n",
    "            labels = [0] * len(self.positive_data)\n",
    "            \n",
    "        self.data = np.concatenate([\n",
    "            np.array(data), \n",
    "            np.array(labels)[:, np.newaxis]], \n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    def sample_negatives(self):\n",
    "        negative_data = []\n",
    "        for user, positive in self.positive_data:\n",
    "            for _ in range(self.negative_samples):\n",
    "                negative = np.random.randint(self.item_num)\n",
    "                while (user, negative) in self.positive_mat:\n",
    "                    negative = np.random.randint(self.item_num)\n",
    "                negative_data.append([user, negative])\n",
    "\n",
    "        return negative_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, item, label = self.data[idx]\n",
    "        output = {\n",
    "            \"user\": user,\n",
    "            \"item\": item,\n",
    "            \"label\": np.float32(label),\n",
    "        }\n",
    "        return output\n",
    "\n",
    "class SamplerWithReset(td.RandomSampler):\n",
    "    def __iter__(self):\n",
    "        self.data_source.reset()\n",
    "        return super().__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "ovEGbITzixZ9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting dataset\n",
      "Resetting dataset\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MLDataset(\n",
    "    train_data, \n",
    "    item_num, \n",
    "    train_mat, \n",
    "    train_negative_samples\n",
    ")\n",
    "train_loader = td.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    sampler=SamplerWithReset(train_dataset)\n",
    ")\n",
    "\n",
    "valid_dataset = MLDataset(valid_data, item_num, train_mat)\n",
    "valid_loader = td.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=test_negative_samples+1, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(tensor):\n",
    "    return tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Li8nNg-3ixZ_"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State_Repr_Module(nn.Module):\n",
    "    def __init__(self, user_num, item_num, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.user_embeddings = nn.Embedding(user_num, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(item_num, embedding_dim)\n",
    "        self.drr_ave = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        nn.init.xavier_uniform_(self.user_embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embeddings.weight)\n",
    "        \n",
    "    def forward(self, user_batch, memory_batch):\n",
    "        user_embedding = self.user_embeddings(user_batch)\n",
    "\n",
    "        item_embeddings = []\n",
    "        for memory in memory_batch:\n",
    "            subitem_embeddings = []\n",
    "            for item_i in memory:\n",
    "                if item_i == -1:\n",
    "                    subitem_embeddings.append(torch.zeros(embedding_dim).to(device))\n",
    "                else:\n",
    "                    subitem_embeddings.append(self.item_embeddings(torch.tensor(int(item_i)).to(device)))\n",
    "            item_embeddings.append(torch.stack(subitem_embeddings))\n",
    "#         set_trace()\n",
    "        drr_ave = self.drr_ave(torch.stack(item_embeddings).permute((0, 2, 1))).squeeze(-1)\n",
    "        \n",
    "        return torch.cat((user_embedding, user_embedding * drr_ave, drr_ave), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Actor_DRR(nn.Module):\n",
    "    def __init__(self, user_num, item_num, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                layer.bias.data.zero_()\n",
    "            \n",
    "    def forward(self, state):\n",
    "        action_embedding = torch.tanh(self.layers(state))\n",
    "        return action_embedding\n",
    "    \n",
    "    def get_action(self, state, state_repr, items=torch.tensor([i for i in range(item_num)])):\n",
    "        pred = torch.bmm(\n",
    "            state_repr.item_embeddings(items).unsqueeze(0), \n",
    "            self.forward(state).T.unsqueeze(0)\n",
    "        )\n",
    "#         set_trace()\n",
    "        return torch.gather(items, 0, pred.squeeze(0).argmax(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Critic_DRR(nn.Module):\n",
    "    def __init__(self, state_repr_dim, action_emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(state_repr_dim + action_emb_dim, hidden_dim)\n",
    "#         self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self, init_w=3e-3):\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "#         x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n"
     ]
    }
   ],
   "source": [
    "state_repr = State_Repr_Module(user_num, item_num, embedding_dim, hidden_dim)\n",
    "value_net =  Critic_DRR(embedding_dim * 3, embedding_dim, hidden_dim).to(device)\n",
    "policy_net  =  Actor_DRR(user_num, item_num, embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "target_value_net =  Critic_DRR(embedding_dim * 3, embedding_dim, hidden_dim).to(device)\n",
    "target_policy_net  = Actor_DRR(user_num, item_num, embedding_dim, hidden_dim).to(device).to(device)\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "value_criterion = nn.MSELoss()\n",
    "value_optimizer      = Ranger(value_net.parameters(),  lr=1e-3)\n",
    "policy_optimizer     = Ranger(policy_net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def ddpg_update(batch_size=16, \n",
    "                gamma = 0.6,\n",
    "                min_value=-np.inf,\n",
    "                max_value=np.inf,\n",
    "                soft_tau=1e-2):\n",
    "    \n",
    "    state, action, reward, next_state = replay_buffer.sample(batch_size)\n",
    "    state      = torch.FloatTensor(state).to(device)\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    action     = torch.FloatTensor(action).to(device)\n",
    "    reward     = torch.FloatTensor(reward).to(device)\n",
    "\n",
    "    policy_loss = value_net(state, policy_net(state))\n",
    "    policy_loss = -policy_loss.mean()\n",
    "\n",
    "    next_action    = target_policy_net(next_state)\n",
    "    target_value   = target_value_net(next_state, next_action.detach())\n",
    "    expected_value = reward + gamma * target_value\n",
    "    expected_value = torch.clamp(expected_value, min_value, max_value)\n",
    "\n",
    "    value = value_net(state, action)\n",
    "    value_loss = value_criterion(value, expected_value.detach())\n",
    "    \n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "                )\n",
    "\n",
    "    for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self):\n",
    "        self.matrix = train_mat\n",
    "        self.max_session_length = session_length\n",
    "        self.item_count = item_num\n",
    "    \n",
    "    def reset(self, user_id):\n",
    "        self.user_id = user_id\n",
    "        self.current_session_length = 0\n",
    "        self.related_items = np.argwhere(self.matrix[self.user_id] > 0)[:session_length, 1]\n",
    "        self.num_rele = len(self.related_items)\n",
    "        self.nonrelated_items = np.random.choice(\n",
    "            list(set(range(self.item_count)) - set(self.related_items)), self.num_rele)\n",
    "        self.available_items = np.zeros(self.num_rele * 2)\n",
    "        self.available_items[::2] = self.related_items\n",
    "        self.available_items[1::2] = self.nonrelated_items\n",
    "        self.viewed_items = []\n",
    "        self.memory = [-1] * 5\n",
    "        \n",
    "        return state_repr(\n",
    "            torch.tensor(self.user_id).unsqueeze(0), \n",
    "            torch.tensor(self.memory).unsqueeze(0)\n",
    "        )\n",
    "    \n",
    "    def step(self, action, new_user_id):\n",
    "        reward = float(to_np(action)[0] in self.related_items)\n",
    "        self.viewed_items.append(to_np(action)[0])\n",
    "        if reward:\n",
    "            self.memory = list(self.memory[1:]) + [action]\n",
    "        \n",
    "        self.current_session_length += 1\n",
    "        if self.current_session_length < self.max_session_length:\n",
    "            next_state = state_repr(\n",
    "                torch.tensor(self.user_id).unsqueeze(0), \n",
    "                torch.tensor(self.memory).unsqueeze(0)\n",
    "            )\n",
    "            replay_buffer.push(to_np(state)[0], to_np(action_emb)[0], np.array([reward]), to_np(next_state)[0])\n",
    "        else:\n",
    "            next_state = self.reset(new_user_id)\n",
    "            \n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:28<00:00,  4.79it/s]\n"
     ]
    }
   ],
   "source": [
    "replay_buffer_size = 100000\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "\n",
    "preds = []\n",
    "rewards = []\n",
    "batch_number = 0\n",
    "appropriate_users = np.arange(user_num).reshape(-1, 1)[train_mat.sum(1) > session_length]\n",
    "users = np.random.permutation(appropriate_users)\n",
    "\n",
    "env = Env()\n",
    "state = env.reset(users[0])\n",
    "for session_id in tqdm.tqdm(range(1000)):\n",
    "    for t in range(20):\n",
    "        action_emb = policy_net(state)\n",
    "        action = policy_net.get_action(\n",
    "            state, \n",
    "            state_repr, \n",
    "            torch.tensor([item for item in env.available_items if item not in env.viewed_items]).long()\n",
    "        )\n",
    "\n",
    "        next_state, reward = env.step(action, new_user_id=users[session_id % len(users)])\n",
    "        state = next_state\n",
    "        \n",
    "        preds.append(action)\n",
    "        rewards.append(reward)\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            ddpg_update()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "s-iWfb5TixZc",
    "baIIDXdAixaH"
   ],
   "name": "pytorch.pipelines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
