{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBbEvG1aixZo"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ranger import Ranger\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as td\n",
    "import torch.optim as to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7YiOW9TixZs"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device= 'cpu'\n",
    "# The directory to store the data\n",
    "data_dir = \"data\"\n",
    "\n",
    "train_rating = \"ml-1m.train.rating\"\n",
    "test_negative = \"ml-1m.test.negative\"\n",
    "\n",
    "train_negative_samples = 4\n",
    "test_negative_samples = 99\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128 # 32\n",
    "N = 10 # memory size for state_repr\n",
    "\n",
    "# Training config\n",
    "batch_size = 16 # 512\n",
    "top_k=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BExS1c1zixZv"
   },
   "source": [
    "## Data\n",
    "\n",
    "\n",
    "Use Movielens 1M data from the https://github.com/hexiangnan/neural_collaborative_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "qabQkULCixZv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip loading ml-1m.train.rating\n",
      "Skip loading ml-1m.test.negative\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "    \n",
    "for file_name in [train_rating, test_negative]:\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"Skip loading \" + file_name)\n",
    "        continue\n",
    "    with open(file_path, \"wb\") as tf:\n",
    "        print(\"Load \" + file_name)\n",
    "        r = requests.get(\"https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/\" + file_name)\n",
    "        tf.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "lDR1YFTbixZx"
   },
   "outputs": [],
   "source": [
    "def preprocess_train():\n",
    "    train_data = pd.read_csv(os.path.join(data_dir, train_rating), sep='\\t', header=None, \n",
    "                             names=['user', 'item', 'rating'], usecols=[0, 1, 2], \n",
    "                             dtype={0: np.int32, 1: np.int32, 2: np.int8})\n",
    "    \n",
    "    train_data = train_data[train_data['rating'] > 3][['user', 'item']]\n",
    "    user_num = train_data['user'].max() + 1\n",
    "    item_num = train_data['item'].max() + 1\n",
    "\n",
    "    mat = defaultdict(int)\n",
    "    train_data = train_data.values.tolist()\n",
    "    for user, item in train_data:\n",
    "        mat[user, item] = 1.0\n",
    "        \n",
    "    # Convert ratings as a dok matrix\n",
    "    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "    dict.update(train_mat, mat)\n",
    "    \n",
    "    return train_data, train_mat, user_num, item_num\n",
    "\n",
    "train_data, train_mat, user_num, item_num = preprocess_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "BGqqEVoRixZ0"
   },
   "outputs": [],
   "source": [
    "def preprocess_test():\n",
    "    test_data = []\n",
    "    with open(os.path.join(data_dir, test_negative)) as tnf:\n",
    "        for line in tnf:\n",
    "            parts = line.split('\\t')\n",
    "            assert len(parts) == test_negative_samples + 1\n",
    "            user, positive = eval(parts[0])\n",
    "            test_data.append([user, positive])\n",
    "            \n",
    "            for negative in parts[1:]:\n",
    "                test_data.append([user, int(negative)])\n",
    "\n",
    "    return test_data\n",
    "\n",
    "valid_data = preprocess_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "RUGNP7u0ixZ3"
   },
   "outputs": [],
   "source": [
    "class MLDataset(td.Dataset):\n",
    "    \n",
    "    def __init__(self, positive_data, item_num, positive_mat, negative_samples=99):\n",
    "        super(MLDataset, self).__init__()\n",
    "        self.positive_data = positive_data\n",
    "        self.item_num = item_num\n",
    "        self.positive_mat = positive_mat\n",
    "        self.negative_samples = negative_samples\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        print(\"Resetting dataset\")\n",
    "        if self.negative_samples > 0:\n",
    "            negative_data = self.sample_negatives()\n",
    "            data = self.positive_data + negative_data\n",
    "            labels = [1] * len(self.positive_data) + [0] * len(negative_data)\n",
    "        else:\n",
    "            data = self.positive_data\n",
    "            labels = [0] * len(self.positive_data)\n",
    "            \n",
    "        self.data = np.concatenate([\n",
    "            np.array(data), \n",
    "            np.array(labels)[:, np.newaxis]], \n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    def sample_negatives(self):\n",
    "        negative_data = []\n",
    "        for user, positive in self.positive_data:\n",
    "            for _ in range(self.negative_samples):\n",
    "                negative = np.random.randint(self.item_num)\n",
    "                while (user, negative) in self.positive_mat:\n",
    "                    negative = np.random.randint(self.item_num)\n",
    "                    \n",
    "                negative_data.append([user, negative])\n",
    "\n",
    "        return negative_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, item, label = self.data[idx]\n",
    "        output = {\n",
    "            \"user\": user,\n",
    "            \"item\": item,\n",
    "            \"label\": np.float32(label),\n",
    "        }\n",
    "        return output\n",
    "\n",
    "class SamplerWithReset(td.RandomSampler):\n",
    "    def __iter__(self):\n",
    "        self.data_source.reset()\n",
    "        return super().__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "ovEGbITzixZ9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting dataset\n",
      "Resetting dataset\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MLDataset(\n",
    "    train_data, \n",
    "    item_num, \n",
    "    train_mat, \n",
    "    train_negative_samples\n",
    ")\n",
    "train_loader = td.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    sampler=SamplerWithReset(train_dataset)\n",
    ")\n",
    "\n",
    "valid_dataset = MLDataset(valid_data, item_num, train_mat)\n",
    "valid_loader = td.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=test_negative_samples+1, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def update_item_representation(memory, user, new_item, label):\n",
    "    if label:\n",
    "        memory[user] = list(memory[user, 1:]) + [new_item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Li8nNg-3ixZ_"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class State_Repr_Module(nn.Module):\n",
    "    def __init__(self, user_num, item_num, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.user_embeddings = nn.Embedding(user_num, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(item_num, embedding_dim)\n",
    "        self.drr_ave = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        nn.init.xavier_uniform_(self.user_embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embeddings.weight)\n",
    "        \n",
    "    def forward(self, user, item, memory):\n",
    "        user_embedding = self.user_embeddings(user)\n",
    "        item_embedding = self.item_embeddings(item)\n",
    "\n",
    "        item_embeddings = []\n",
    "        for u, i in zip(user, item):\n",
    "            subitem_embeddings = []\n",
    "            for item_i in memory[u.cpu().numpy()]:\n",
    "                if item_i == -1:\n",
    "                    subitem_embeddings.append(torch.zeros(embedding_dim).to(device))\n",
    "                else:\n",
    "                    subitem_embeddings.append(self.item_embeddings(torch.tensor(int(item_i)).to(device)))\n",
    "            item_embeddings.append(torch.stack(subitem_embeddings))\n",
    "\n",
    "        drr_ave = self.drr_ave(torch.stack(item_embeddings).permute((0, 2, 1))).squeeze(-1)\n",
    "        \n",
    "        return torch.cat((user_embedding, user_embedding * drr_ave, drr_ave), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Actor_DRR(nn.Module):\n",
    "    def __init__(self, user_num, item_num, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                layer.bias.data.zero_()\n",
    "            \n",
    "    def forward(self, state):\n",
    "        action_embedding = torch.tanh(self.layers(state))\n",
    "        return action_embedding\n",
    "    \n",
    "    def get_action(self, state, state_repr, items=torch.tensor([i for i in range(item_num)])):\n",
    "        pred = torch.bmm(\n",
    "            state_repr.item_embeddings(items).unsqueeze(0), \n",
    "            self.forward(state).T.unsqueeze(0)\n",
    "        )\n",
    "        return pred.squeeze(0).argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Critic_DRR(nn.Module):\n",
    "    def __init__(self, state_repr_dim, action_emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_repr_dim + action_emb_dim, hidden_dim)\n",
    "#         self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self, init_w=3e-3):\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "#         x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "state_repr = State_Repr_Module(user_num, item_num, embedding_dim, hidden_dim)\n",
    "value_net =  Critic_DRR(embedding_dim * 3, embedding_dim, hidden_dim).to(device)\n",
    "policy_net  =  Actor_DRR(user_num, item_num, embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "target_value_net =  Critic_DRR(embedding_dim * 3, embedding_dim, hidden_dim).to(device)\n",
    "target_policy_net  = Actor_DRR(user_num, item_num, embedding_dim, hidden_dim).to(device).to(device)\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "value_criterion = nn.MSELoss()\n",
    "value_optimizer      = Ranger(value_net.parameters(),  lr=1e-4)\n",
    "policy_optimizer     = Ranger(policy_net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def ddpg_update(batch_size=16, \n",
    "                gamma = 0.6,\n",
    "                min_value=-np.inf,\n",
    "                max_value=np.inf,\n",
    "                soft_tau=1e-2):\n",
    "    \n",
    "    state, action, reward, next_state = replay_buffer.sample(batch_size)\n",
    "    state      = torch.FloatTensor(state).to(device)\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    action     = torch.FloatTensor(action).to(device)\n",
    "    reward     = torch.FloatTensor(reward).to(device)\n",
    "\n",
    "    policy_loss = value_net(state, policy_net(state))\n",
    "    policy_loss = -policy_loss.mean()\n",
    "\n",
    "    next_action    = target_policy_net(next_state)\n",
    "    target_value   = target_value_net(next_state, next_action.detach())\n",
    "    expected_value = reward + gamma * target_value\n",
    "    expected_value = torch.clamp(expected_value, min_value, max_value)\n",
    "\n",
    "    value = value_net(state, action)\n",
    "    value_loss = value_criterion(value, expected_value.detach())\n",
    "    \n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "                )\n",
    "\n",
    "    for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting dataset\n"
     ]
    }
   ],
   "source": [
    "replay_buffer_size = 10000\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "\n",
    "memory = np.ones([user_num, N]) * -1\n",
    "it = iter(train_loader)\n",
    "batch = next(it)\n",
    "user, item, label = batch[\"user\"], batch[\"item\"], batch[\"label\"]\n",
    "state = state_repr(user, item, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 132.29it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "\n",
    "for j in tqdm.tqdm(range(1000)):\n",
    "    action_emb = policy_net(state)\n",
    "    action = policy_net.get_action(state, state_repr)\n",
    "    preds.append(action)\n",
    "    \n",
    "    reward = torch.tensor(train_mat[list(user.detach().cpu().numpy()), list(action.detach().cpu().numpy())].toarray()[0])\n",
    "#     for u, i, r in zip(user, item, reward):\n",
    "#         update_item_representation(memory, u, i, r)\n",
    "    update_item_representation(memory, user, item, reward)\n",
    "\n",
    "    next_batch = next(it)\n",
    "    user, item, label = next_batch[\"user\"], next_batch[\"item\"], next_batch[\"label\"]\n",
    "    next_state = state_repr(user, item, memory)\n",
    "\n",
    "    replay_buffer.push(state.detach().cpu().numpy()[0], action_emb.detach().cpu().numpy()[0], \n",
    "                       reward, next_state.detach().cpu().numpy()[0])\n",
    "    if len(replay_buffer) > batch_size:\n",
    "        ddpg_update()\n",
    "\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting dataset\n"
     ]
    }
   ],
   "source": [
    "it2 = iter(td.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    sampler=SamplerWithReset(train_dataset)\n",
    "))\n",
    "batch = next(it2)\n",
    "user, item, label = batch[\"user\"], batch[\"item\"], batch[\"label\"]\n",
    "state = state_repr(user, item, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2640, 3840, 3107, 4841, 5949, 2763,  686, 2592, 5442, 4020, 3723, 1604,\n",
       "        2914, 3894, 3329, 3386])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = policy_net.get_action(state, state_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2859, 2859, 2859, 2859,  856, 2859, 2859, 2859, 2859,  856,  856, 2859,\n",
       "        2859, 2859, 2859,  856])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель постепенно сходится к одинаковым предсказаниям. \n",
    "Это сохраняется при уменьшении `lr` (число итераций до сходимости увеличивается пропорционально уменьшению `lr`)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "s-iWfb5TixZc",
    "baIIDXdAixaH"
   ],
   "name": "pytorch.pipelines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
