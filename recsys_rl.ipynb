{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBbEvG1aixZo"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ranger import Ranger\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as to\n",
    "import torch.utils.data as td\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7YiOW9TixZs"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device= 'cpu'\n",
    "\n",
    "# The directory to store the data\n",
    "data_dir = \"data\"\n",
    "train_rating = \"ml-1m.train.rating\"\n",
    "test_negative = \"ml-1m.test.negative\"\n",
    "\n",
    "batch_size = 32\n",
    "session_length = 60\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128 # 32\n",
    "N = 5 # memory size for state_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BExS1c1zixZv"
   },
   "source": [
    "## Data\n",
    "\n",
    "\n",
    "Use Movielens 1M data from the https://github.com/hexiangnan/neural_collaborative_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "qabQkULCixZv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip loading ml-1m.train.rating\n",
      "Skip loading ml-1m.test.negative\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "    \n",
    "for file_name in [train_rating, test_negative]:\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"Skip loading \" + file_name)\n",
    "        continue\n",
    "    with open(file_path, \"wb\") as tf:\n",
    "        print(\"Load \" + file_name)\n",
    "        r = requests.get(\"https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/\" + file_name)\n",
    "        tf.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "lDR1YFTbixZx"
   },
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    data = pd.read_csv(os.path.join(data_dir, train_rating), sep='\\t', header=None, \n",
    "                             names=['user', 'item', 'rating'], usecols=[0, 1, 2], \n",
    "                             dtype={0: np.int32, 1: np.int32, 2: np.int8})\n",
    "    data = data[data['rating'] > 3][['user', 'item']]\n",
    "    user_num = data['user'].max() + 1\n",
    "    item_num = data['item'].max() + 1\n",
    "\n",
    "    train_mat = defaultdict(int)\n",
    "    test_mat = defaultdict(int)\n",
    "    \n",
    "    train_data = data.sample(frac=0.8, random_state=16)\n",
    "    test_data = data.drop(train_data.index).values.tolist()\n",
    "    train_data = train_data.values.tolist()\n",
    "    \n",
    "    for user, item in train_data:\n",
    "        train_mat[user, item] = 1.0\n",
    "    for user, item in test_data:\n",
    "        test_mat[user, item] = 1.0\n",
    "        \n",
    "    # Convert ratings as a dok matrix\n",
    "    train_matrix = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "    dict.update(train_matrix, train_mat)\n",
    "    test_matrix = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "    dict.update(test_matrix, test_mat)\n",
    "    \n",
    "    return train_data, train_matrix, test_data, test_matrix, user_num, item_num\n",
    "\n",
    "train_data, train_matrix, test_data, test_matrix, user_num, item_num = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     29
    ],
    "colab": {},
    "colab_type": "code",
    "id": "RUGNP7u0ixZ3"
   },
   "outputs": [],
   "source": [
    "class EvalDataset(td.Dataset):\n",
    "    \n",
    "    def __init__(self, positive_data, item_num, positive_mat, negative_samples=99, valid=True):\n",
    "        super(EvalDataset, self).__init__()\n",
    "        self.positive_data = np.array(positive_data)\n",
    "        self.item_num = item_num\n",
    "        self.positive_mat = positive_mat\n",
    "        self.negative_samples = negative_samples\n",
    "        self.valid = valid\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        print(\"Resetting dataset\")\n",
    "        negative_data = self.sample_negatives()\n",
    "        if self.valid:\n",
    "            data = self.create_valid_data()\n",
    "            labels = np.zeros(len(self.positive_data) * (1 + self.negative_samples))\n",
    "            labels[::1+self.negative_samples] = 1\n",
    "        else:\n",
    "            data = self.positive_data + negative_data\n",
    "            labels = [1] * len(self.positive_data) + [0] * len(negative_data)\n",
    "            \n",
    "        self.data = np.concatenate([\n",
    "            np.array(data), \n",
    "            np.array(labels)[:, np.newaxis]], \n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    def sample_negatives(self):\n",
    "        negative_data = []\n",
    "        for user, positive in self.positive_data:\n",
    "            for _ in range(self.negative_samples):\n",
    "                negative = np.random.randint(self.item_num)\n",
    "                while (user, negative) in self.positive_mat:\n",
    "                    negative = np.random.randint(self.item_num)\n",
    "                negative_data.append([user, negative])\n",
    "\n",
    "        return negative_data\n",
    "\n",
    "    def create_valid_data(self):\n",
    "        valid_data = []\n",
    "        for user, positive in self.positive_data:\n",
    "            valid_data.append([user, positive])\n",
    "            for i in range(self.negative_samples):\n",
    "                negative = np.random.randint(self.item_num)\n",
    "                while (user, negative) in self.positive_mat:\n",
    "                    negative = np.random.randint(self.item_num)\n",
    "                    \n",
    "                valid_data.append([user, negative])\n",
    "\n",
    "        return valid_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, item, label = self.data[idx]\n",
    "        output = {\n",
    "            \"user\": user,\n",
    "            \"item\": item,\n",
    "            \"label\": np.float32(label),\n",
    "        }\n",
    "        return output\n",
    "\n",
    "class SamplerWithReset(td.RandomSampler):\n",
    "    def __iter__(self):\n",
    "        self.data_source.reset()\n",
    "        return super().__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "ovEGbITzixZ9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting dataset\n"
     ]
    }
   ],
   "source": [
    "# just one user for evaluation speed\n",
    "valid_dataset = EvalDataset(\n",
    "    np.array(test_data)[np.array(test_data)[:, 0] == 6039], \n",
    "    item_num, \n",
    "    test_matrix)\n",
    "\n",
    "valid_loader = td.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=100, \n",
    "    shuffle=False, \n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class NaivePrioritizedBuffer(object):\n",
    "    def __init__(self, capacity, prob_alpha=0.6):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity   = capacity\n",
    "        self.buffer     = []\n",
    "        self.pos        = 0\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state):\n",
    "        assert state.ndim == next_state.ndim\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        \n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state)\n",
    "        \n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        \n",
    "        probs  = prios ** self.prob_alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        total    = len(self.buffer)\n",
    "        weights  = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights  = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        batch       = list(zip(*samples))\n",
    "        states      = np.concatenate(batch[0])\n",
    "        actions     = batch[1]\n",
    "        rewards     = batch[2]\n",
    "        next_states = np.concatenate(batch[3])\n",
    "        \n",
    "        return states, actions, rewards, next_states\n",
    "    \n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_start = 0.4\n",
    "beta_steps = 10000\n",
    "\n",
    "def get_beta(idx):\n",
    "    return min(1.0, beta_start + idx * (1.0 - beta_start) / beta_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(tensor):\n",
    "    return tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Li8nNg-3ixZ_"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State_Repr_Module(nn.Module):\n",
    "    def __init__(self, user_num, item_num, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.user_embeddings = nn.Embedding(user_num, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(item_num+1, embedding_dim, padding_idx=int(item_num))\n",
    "        self.drr_ave = torch.nn.Conv1d(in_channels=N, out_channels=1, kernel_size=1) # nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.initialize()\n",
    "            \n",
    "    def initialize(self):\n",
    "        nn.init.normal_(self.user_embeddings.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embeddings.weight, std=0.01)\n",
    "        self.item_embeddings.weight.data[-1].zero_()\n",
    "        nn.init.xavier_uniform_(self.drr_ave.weight)\n",
    "        self.drr_ave.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, user, memory):\n",
    "        user_embedding = self.user_embeddings(user)\n",
    "        \n",
    "        item_embeddings = self.item_embeddings(memory.long())\n",
    "        drr_ave = self.drr_ave(item_embeddings).squeeze(1)\n",
    "        \n",
    "        return torch.cat((user_embedding, user_embedding * drr_ave, drr_ave), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Actor_DRR(nn.Module):\n",
    "    def __init__(self, user_num, item_num, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(layer.weight)\n",
    "            \n",
    "    def forward(self, state):\n",
    "        return self.layers(state)\n",
    "    \n",
    "    def get_action(self, state, state_repr, \n",
    "                   items=torch.tensor([i for i in range(item_num)]), \n",
    "                   return_scores=False):\n",
    "        pred = torch.bmm(\n",
    "            state_repr.item_embeddings(items).unsqueeze(0), \n",
    "            self.forward(state).T.unsqueeze(0)\n",
    "        )\n",
    "        if return_scores:\n",
    "            return pred.squeeze(0), torch.gather(items, 0, pred.squeeze(0).argmax(0))\n",
    "        else:\n",
    "            return torch.gather(items, 0, pred.squeeze(0).argmax(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Critic_DRR(nn.Module):\n",
    "    def __init__(self, state_repr_dim, action_emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_repr_dim + action_emb_dim, hidden_dim), \n",
    "            nn.ReLU(), \n",
    "#             nn.Linear(hidden_dim, hidden_dim), \n",
    "#             nn.ReLU(), \n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(layer.weight)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self, user_item_matrix, session_length=session_length):\n",
    "        self.matrix = user_item_matrix\n",
    "        self.max_session_length = session_length\n",
    "        self.item_count = item_num\n",
    "        self.memory = np.ones([user_num, N]) * item_num\n",
    "\n",
    "    def reset(self, user_id):\n",
    "        self.user_id = user_id\n",
    "        self.current_session_length = 0\n",
    "        self.related_items = np.argwhere(self.matrix[self.user_id] > 0)[:, 1]\n",
    "        self.num_rele = len(self.related_items)\n",
    "        self.nonrelated_items = np.random.choice(\n",
    "            list(set(range(self.item_count)) - set(self.related_items)), self.num_rele)\n",
    "        self.available_items = np.zeros(self.num_rele * 2)\n",
    "        self.available_items[::2] = self.related_items\n",
    "        self.available_items[1::2] = self.nonrelated_items\n",
    "        self.viewed_items = []\n",
    "        \n",
    "        return state_repr(\n",
    "            torch.tensor(self.user_id).unsqueeze(0).to(device), \n",
    "            torch.tensor(self.memory[self.user_id]).unsqueeze(0).to(device)\n",
    "        )\n",
    "    \n",
    "    def step(self, action, action_emb, new_user_id, buffer=None):\n",
    "        reward = float(to_np(action)[0] in self.related_items)\n",
    "        self.viewed_items.append(to_np(action)[0])\n",
    "        if reward:\n",
    "            if len(action) == 1:\n",
    "                self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [action]\n",
    "            else:\n",
    "                self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [action[0]]\n",
    "                \n",
    "        self.current_session_length += 1\n",
    "        if (self.current_session_length >= self.max_session_length or \n",
    "            self.current_session_length >= self.num_rele):\n",
    "            next_state = self.reset(new_user_id)\n",
    "        else:\n",
    "            next_state = state_repr(\n",
    "                torch.tensor(self.user_id).unsqueeze(0).to(device), \n",
    "                torch.tensor(self.memory[self.user_id]).unsqueeze(0).to(device)\n",
    "            )\n",
    "            if buffer is not None:\n",
    "                buffer.push(to_np(state)[0], to_np(action_emb)[0], np.array([reward]), to_np(next_state)[0])\n",
    "            \n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_metric(recommended, actual):\n",
    "    return int(actual in recommended)\n",
    "\n",
    "def dcg_metric(recommended, actual):\n",
    "    if actual in recommended:\n",
    "        index = recommended.index(actual)\n",
    "        return np.reciprocal(np.log2(index + 2))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(net, state_representation, training_env):\n",
    "    hits = []\n",
    "    dcgs = []\n",
    "    test_env = Env(test_matrix, session_length=len(valid_loader))\n",
    "    test_env.memory = training_env.memory.copy()\n",
    "    test_env.reset(6039)\n",
    "    for batch in valid_loader:\n",
    "        s = state_representation(\n",
    "            batch['user'].long().to(device), \n",
    "            torch.tensor(test_env.memory[to_np(batch['user']).astype(int), :]).to(device))\n",
    "        scores, action = net.get_action(\n",
    "            s, state_repr, \n",
    "            batch['item'].long().to(device), \n",
    "            return_scores=True\n",
    "        )\n",
    "        _, reward = test_env.step(action, None, new_user_id=6039)\n",
    "\n",
    "        _, ind = F.softmax(scores[:, 0], 0).topk(10)\n",
    "        predictions = torch.take(batch['item'].to(device), ind).cpu().numpy().tolist()\n",
    "        actual = batch['item'][0].item()\n",
    "        hits.append(hit_metric(predictions, actual))\n",
    "        dcgs.append(dcg_metric(predictions, actual))\n",
    "        \n",
    "    return np.mean(hits), np.mean(dcgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n"
     ]
    }
   ],
   "source": [
    "state_repr = State_Repr_Module(user_num, item_num, embedding_dim, hidden_dim).to(device)\n",
    "value_net  = Critic_DRR(embedding_dim * 3, embedding_dim, hidden_dim).to(device)\n",
    "policy_net = Actor_DRR(user_num, item_num, embedding_dim, hidden_dim).to(device)\n",
    "replay_buffer = NaivePrioritizedBuffer(1000000)\n",
    "\n",
    "target_value_net  = Critic_DRR(embedding_dim * 3, embedding_dim, hidden_dim).to(device)\n",
    "target_policy_net = Actor_DRR(user_num, item_num, embedding_dim, hidden_dim).to(device).to(device)\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "value_criterion  = nn.MSELoss()\n",
    "value_optimizer  = Ranger(value_net.parameters(),  lr=1e-4)\n",
    "policy_optimizer = Ranger(policy_net.parameters(), lr=1e-4)\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs/log4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def ddpg_update(training_env, \n",
    "                step=0,\n",
    "                batch_size=batch_size, \n",
    "                gamma=0.8,\n",
    "                min_value=-100,\n",
    "                max_value=100,\n",
    "                soft_tau=0.001,\n",
    "               ):\n",
    "    beta = get_beta(step)\n",
    "    state, action, reward, next_state = replay_buffer.sample(batch_size, beta)\n",
    "    state      = torch.FloatTensor(state).to(device)\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    action     = torch.FloatTensor(action).to(device)\n",
    "    reward     = torch.FloatTensor(reward).to(device)\n",
    "\n",
    "    policy_loss = value_net(state, policy_net(state))\n",
    "    policy_loss = -policy_loss.mean()\n",
    "\n",
    "    next_action    = target_policy_net(next_state)\n",
    "    target_value   = target_value_net(next_state, next_action.detach())\n",
    "    expected_value = reward + gamma * target_value\n",
    "    expected_value = torch.clamp(expected_value, min_value, max_value)\n",
    "\n",
    "    value = value_net(state, action)\n",
    "    value_loss = value_criterion(value, expected_value.detach())\n",
    "    \n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "                )\n",
    "\n",
    "    for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n",
    "    \n",
    "    writer.add_histogram('value', value, step)\n",
    "    writer.add_histogram('target_value', target_value, step)\n",
    "    writer.add_histogram('expected_value', expected_value, step)\n",
    "    writer.add_histogram('policy_loss', policy_loss, step)\n",
    "    if step % 100 == 0 and step > 0:\n",
    "        hit, dcg = run_evaluation(policy_net, state_repr, training_env)\n",
    "        writer.add_scalar('hit', hit, step)\n",
    "        writer.add_scalar('dcg', dcg, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4699/4699 [1:12:07<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "train_env = Env(train_matrix)\n",
    "appropriate_users = np.arange(user_num).reshape(-1, 1)[(train_matrix.sum(1) >= 20)]\n",
    "users = np.random.permutation(appropriate_users)\n",
    "state = train_env.reset(users[0])\n",
    "step = 0\n",
    "\n",
    "for user_id in tqdm.tqdm(range(len(users))):\n",
    "    for t in range(int(train_matrix[user_id].sum())):\n",
    "        action_emb = policy_net(state)\n",
    "        action = policy_net.get_action(\n",
    "            state, \n",
    "            state_repr, \n",
    "            torch.tensor([item for item in train_env.available_items \n",
    "                          if item not in train_env.viewed_items]).long().to(device)\n",
    "        )\n",
    "\n",
    "        next_state, reward = train_env.step(\n",
    "            action, \n",
    "            action_emb,\n",
    "            new_user_id=users[(user_id + 1) % len(users)], \n",
    "            buffer=replay_buffer\n",
    "        )\n",
    "        state = next_state\n",
    "        \n",
    "        if len(replay_buffer) > batch_size:\n",
    "            ddpg_update(train_env, step=step)\n",
    "        step += 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "s-iWfb5TixZc",
    "baIIDXdAixaH"
   ],
   "name": "pytorch.pipelines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
