{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBbEvG1aixZo"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as td\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam\n",
    "\n",
    "from utils import (EvalDataset, OUNoise, Prioritized_Buffer, get_beta, \n",
    "                   preprocess_data, to_np, hit_metric, dcg_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/beer_reviews.csv\")\n",
    "# df = df[[\"beer_beerid\", \"review_profilename\",\"review_overall\",\"review_aroma\",\"review_appearance\",\"review_palate\",\"review_taste\",]]\n",
    "# df = df.dropna(axis=0)\n",
    "#\n",
    "# df[\"beer_id\"] = df.beer_beerid.astype('category').cat.codes\n",
    "# df[\"reviewer_id\"] = df.review_profilename.astype('category').cat.codes\n",
    "#\n",
    "# df = df.drop(\"beer_beerid\", axis=1)\n",
    "# df = df.drop(\"review_profilename\", axis=1)\n",
    "#\n",
    "# df = df.rename(columns={\"review_overall\":\"overall\",\n",
    "#                    \"review_aroma\":\"aroma\",\n",
    "#                    \"review_appearance\":\"appearance\",\n",
    "#                    \"review_palate\":\"palate\",\n",
    "#                    \"review_taste\":\"taste\"}\n",
    "#           )\n",
    "#\n",
    "# df.to_csv(\"data/clean_beer_reviews.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7YiOW9TixZs"
   },
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "rating = \"clean_beer_reviews.csv\"\n",
    "\n",
    "params = {\n",
    "    'batch_size': 512,\n",
    "    'embedding_dim': 8,\n",
    "    'hidden_dim': 16,\n",
    "    'N': 5, # memory size for state_repr\n",
    "    'ou_noise':False,\n",
    "    \n",
    "    'value_lr': 1e-5,\n",
    "    'value_decay': 1e-4,\n",
    "    'policy_lr': 1e-5,\n",
    "    'policy_decay': 1e-6,\n",
    "    'state_repr_lr': 1e-5,\n",
    "    'state_repr_decay': 1e-3,\n",
    "    'log_dir': 'logs/final/',\n",
    "    'gamma': 0.8,\n",
    "    'min_value': -10,\n",
    "    'max_value': 10,\n",
    "    'soft_tau': 1e-3,\n",
    "    \n",
    "    'buffer_size': 1000000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional recommendation task can be treated as sequental desicion making problem.\n",
    "Recommender (i.e. agent) interacts with users (i.e. environment) to sequentally suggest set of items.\n",
    "The goal is to maximize clients' satisfaction (i.e. reward).\n",
    "More specifically:\n",
    "- State is a vector $a \\in R^{3\\cdot embedding\\_dim}$ computed using the user embedding and the embeddings of `N` latest positive interactions. In the code (replay buffer) state is represented  by `(user, memory)`\n",
    "- Action is a vector $a \\in R^{embedding\\_dim}$. To get ranking score we took dot product of\n",
    "the action and the item embedding (similar to word2vec and other embedding models).\n",
    "- Reward is taken from user-item matrix (1 if rating > 3, 0 otherwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning can help recommendation at least in 2 ways.\n",
    "1. User’s preference on previous items will affect his choice on the next items. \n",
    "User tends to give a higher rating if he has consecutively received more satisfied items (and vice versa). \n",
    "So, it would be more reasonable to model the recommendation as a sequential decision making process.\n",
    "2. It is important to use long-term planning in recommendations. For example, after reading the weather forecast, the user is not willing to read similar news. On the other hand, after watching funny videos or reading memes the user can constanly do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def preprocess_milti_data(data_dir, train_rating):\n",
    "    data = pd.read_csv(os.path.join(data_dir, train_rating))\n",
    "    data = data[['reviewer_id', 'beer_id', \"overall\",\"aroma\",\"appearance\",\"palate\",\"taste\"]]\n",
    "\n",
    "    user_num = data['reviewer_id'].max() + 1\n",
    "    item_num = data['beer_id'].max() + 1\n",
    "\n",
    "    train_data = data.sample(frac=0.8, random_state=16)\n",
    "    test_data = data.drop(train_data.index)\n",
    "\n",
    "    all_aspect_train_matrix = {}\n",
    "    all_aspect_test_matrix = {}\n",
    "\n",
    "    for aspect in [\"overall\",\"aroma\",\"appearance\",\"palate\",\"taste\"]:\n",
    "        aspect_train_data = train_data[train_data[aspect] > 3][['reviewer_id', 'beer_id']].values.tolist()\n",
    "        aspect_test_data = test_data[test_data[aspect] > 3][['reviewer_id', 'beer_id']].values.tolist()\n",
    "\n",
    "        train_matrix = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "        for user, item in aspect_train_data:\n",
    "            train_matrix[user, item] = 1.0\n",
    "        test_matrix = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "        for user, item in aspect_test_data:\n",
    "            test_matrix[user, item] = 1.0\n",
    "\n",
    "        all_aspect_train_matrix[aspect] = train_matrix\n",
    "        all_aspect_test_matrix[aspect] = test_matrix\n",
    "\n",
    "\n",
    "    appropriate_users = np.arange(user_num).reshape(-1, 1)[(all_aspect_train_matrix[\"overall\"].sum(1) >= 20)]\n",
    "\n",
    "    return all_aspect_train_matrix, all_aspect_test_matrix, user_num, item_num, appropriate_users"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aspect_num = 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "qabQkULCixZv"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [48], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m all_aspect_train_matrix, all_aspect_test_matrix, user_num, item_num, appropriate_users\\\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess_milti_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrating\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [44], line 20\u001B[0m, in \u001B[0;36mpreprocess_milti_data\u001B[0;34m(data_dir, train_rating)\u001B[0m\n\u001B[1;32m     18\u001B[0m train_matrix \u001B[38;5;241m=\u001B[39m sp\u001B[38;5;241m.\u001B[39mdok_matrix((user_num, item_num), dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m user, item \u001B[38;5;129;01min\u001B[39;00m aspect_train_data:\n\u001B[0;32m---> 20\u001B[0m     train_matrix[user, item] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m\n\u001B[1;32m     21\u001B[0m test_matrix \u001B[38;5;241m=\u001B[39m sp\u001B[38;5;241m.\u001B[39mdok_matrix((user_num, item_num), dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m user, item \u001B[38;5;129;01min\u001B[39;00m aspect_test_data:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/multi_recom/lib/python3.8/site-packages/scipy/sparse/_index.py:97\u001B[0m, in \u001B[0;36mIndexMixin.__setitem__\u001B[0;34m(self, key, x)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__setitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key, x):\n\u001B[0;32m---> 97\u001B[0m     row, col \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_indices\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(row, INT_TYPES) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, INT_TYPES):\n\u001B[1;32m    100\u001B[0m         x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(x, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/multi_recom/lib/python3.8/site-packages/scipy/sparse/_index.py:152\u001B[0m, in \u001B[0;36mIndexMixin._validate_indices\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    149\u001B[0m M, N \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m    150\u001B[0m row, col \u001B[38;5;241m=\u001B[39m _unpack_index(key)\n\u001B[0;32m--> 152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43misintlike\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    153\u001B[0m     row \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(row)\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m row \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m-\u001B[39mM \u001B[38;5;129;01mor\u001B[39;00m row \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m M:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/multi_recom/lib/python3.8/site-packages/scipy/sparse/_sputils.py:218\u001B[0m, in \u001B[0;36misintlike\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;124;03m\"\"\"Is x appropriate as an index into a sparse matrix? Returns True\u001B[39;00m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;124;03mif it can be cast safely to a machine int.\u001B[39;00m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;66;03m# Fast-path check to eliminate non-scalar values. operator.index would\u001B[39;00m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;66;03m# catch this case too, but the exception catching is slow.\u001B[39;00m\n\u001B[0;32m--> 218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mndim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m<__array_function__ internals>:179\u001B[0m, in \u001B[0;36mndim\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/multi_recom/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3117\u001B[0m, in \u001B[0;36m_ndim_dispatcher\u001B[0;34m(a)\u001B[0m\n\u001B[1;32m   3055\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3056\u001B[0m \u001B[38;5;124;03m    Return the cumulative product of elements along a given axis.\u001B[39;00m\n\u001B[1;32m   3057\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3112\u001B[0m \n\u001B[1;32m   3113\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   3114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapfunc(a, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcumprod\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39maxis, dtype\u001B[38;5;241m=\u001B[39mdtype, out\u001B[38;5;241m=\u001B[39mout)\n\u001B[0;32m-> 3117\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_ndim_dispatcher\u001B[39m(a):\n\u001B[1;32m   3118\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (a,)\n\u001B[1;32m   3121\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_ndim_dispatcher)\n\u001B[1;32m   3122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mndim\u001B[39m(a):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "all_aspect_train_matrix, all_aspect_test_matrix, user_num, item_num, appropriate_users\\\n",
    "    = preprocess_milti_data(data_dir, rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Observation space**. As mentioned before, to get state we need `N` latest positive items (`memory`) and embedding of user. `State_Repr_Module` transform it to the vector of dimensionality `embedding_dim * 3`.\n",
    "\n",
    "- **Action space**. For every user we sample nonrelated items (the same count as related). All `available_items` which wasn't viewed before form action space.\n",
    "\n",
    "Given a state we get action embedding, compute dot product between this embedding and embeddings of all items in action space, take 1 top ranked item, compute reward, update `viewed_items` and memory, and store transition in buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self, user_item_matrix):\n",
    "        self.matrix = user_item_matrix\n",
    "        self.item_count = item_num\n",
    "        self.memory = np.ones([user_num, params['N']]) * item_num\n",
    "        # memory is initialized as [item_num] * N for each user\n",
    "        # it is padding indexes in state_repr and will result in zero embeddings\n",
    "\n",
    "    def reset(self, user_id):\n",
    "        self.user_id = user_id\n",
    "        self.viewed_items = []\n",
    "        self.related_items = np.argwhere(self.matrix[self.user_id] > 0)[:, 1]\n",
    "        self.num_rele = len(self.related_items)\n",
    "        self.nonrelated_items = np.random.choice(\n",
    "            list(set(range(self.item_count)) - set(self.related_items)), self.num_rele)\n",
    "        self.available_items = np.zeros(self.num_rele * 2)\n",
    "        self.available_items[::2] = self.related_items\n",
    "        self.available_items[1::2] = self.nonrelated_items\n",
    "        \n",
    "        return torch.tensor([self.user_id]), torch.tensor(self.memory[[self.user_id], :])\n",
    "    \n",
    "    def step(self, action, action_emb=None, buffer=None):\n",
    "        initial_user = self.user_id\n",
    "        initial_memory = self.memory[[initial_user], :]\n",
    "        \n",
    "        reward = float(to_np(action)[0] in self.related_items)\n",
    "        self.viewed_items.append(to_np(action)[0])\n",
    "        if reward:\n",
    "            if len(action) == 1:\n",
    "                self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [action]\n",
    "            else:\n",
    "                self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [action[0]]\n",
    "                \n",
    "        if len(self.viewed_items) == len(self.related_items):\n",
    "            done = 1\n",
    "        else:\n",
    "            done = 0\n",
    "            \n",
    "        if buffer is not None:\n",
    "            buffer.push(np.array([initial_user]), np.array(initial_memory), to_np(action_emb)[0], \n",
    "                        np.array([reward]), np.array([self.user_id]), self.memory[[self.user_id], :], np.array([reward]))\n",
    "\n",
    "        return torch.tensor([self.user_id]), torch.tensor(self.memory[[self.user_id], :]), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Li8nNg-3ixZ_"
   },
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall model\n",
    "\n",
    "<img src=\"img/full_model.png\" width=\"1000\" height=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Actor_DRR(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(layer.weight)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.layers(state)\n",
    "    \n",
    "    def get_action(self, user, memory, state_repr, \n",
    "                   action_emb,\n",
    "                   items=torch.tensor([i for i in range(item_num)]),\n",
    "                   return_scores=False\n",
    "                  ):\n",
    "        state = state_repr(user, memory)\n",
    "        scores = torch.bmm(state_repr.item_embeddings(items).unsqueeze(0), \n",
    "                         action_emb.T.unsqueeze(0)).squeeze(0)\n",
    "        if return_scores:\n",
    "            return scores, torch.gather(items, 0, scores.argmax(0))\n",
    "        else:\n",
    "            return torch.gather(items, 0, scores.argmax(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Critic_DRR(nn.Module):\n",
    "    def __init__(self, state_repr_dim, action_emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_repr_dim + action_emb_dim, hidden_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(layer.weight)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State representation\n",
    "\n",
    "<img src=\"img/state_representation.png\" width=\"1400\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class State_Repr_Module(nn.Module):\n",
    "    def __init__(self, user_num, item_num, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.user_embeddings = nn.Embedding(user_num, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(item_num+1, embedding_dim, padding_idx=int(item_num))\n",
    "        self.drr_ave = torch.nn.Conv1d(in_channels=params['N'], out_channels=1, kernel_size=1)\n",
    "        \n",
    "        self.initialize()\n",
    "            \n",
    "    def initialize(self):\n",
    "        nn.init.normal_(self.user_embeddings.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embeddings.weight, std=0.01)\n",
    "        self.item_embeddings.weight.data[-1].zero_()\n",
    "        nn.init.uniform_(self.drr_ave.weight)\n",
    "        self.drr_ave.bias.data.zero_()\n",
    "\n",
    "    def forward(self, user, memory):\n",
    "        user_embedding = self.user_embeddings(user.long())\n",
    "\n",
    "        item_embeddings = self.item_embeddings(memory.long())\n",
    "        drr_ave = self.drr_ave(item_embeddings).squeeze(1)\n",
    "        \n",
    "        return torch.cat((user_embedding, user_embedding * drr_ave, drr_ave), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation we take 1 positive and 99 sampled negatives items per batch, select 10 items with best scores and calculate hit_rate@10 and nDCG@10.\n",
    "During training we choose user 6039 and track `hit` and `dcg` only for him (for evaluation speed). Final scores was computed on the whole test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting dataset\n",
      "Resetting dataset\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = EvalDataset(\n",
    "    np.array(test_data)[np.array(test_data)[:, 0] == 6039], \n",
    "    item_num, \n",
    "    test_matrix)\n",
    "valid_loader = td.DataLoader(valid_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "full_dataset = EvalDataset(np.array(test_data), item_num, test_matrix)\n",
    "full_loader = td.DataLoader(full_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(net, state_representation, training_env_memory, loader=valid_loader):\n",
    "    hits = []\n",
    "    dcgs = []\n",
    "    test_env = Env(test_matrix)\n",
    "    test_env.memory = training_env_memory.copy()\n",
    "    user, memory = test_env.reset(int(to_np(next(iter(valid_loader))['user'])[0]))\n",
    "    for batch in loader:\n",
    "        action_emb = net(state_repr(user, memory))\n",
    "        scores, action = net.get_action(\n",
    "            batch['user'], \n",
    "            torch.tensor(test_env.memory[to_np(batch['user']).astype(int), :]), \n",
    "            state_representation, \n",
    "            action_emb,\n",
    "            batch['item'].long(), \n",
    "            return_scores=True\n",
    "        )\n",
    "        user, memory, reward, done = test_env.step(action)\n",
    "\n",
    "        _, ind = scores[:, 0].topk(10)\n",
    "        predictions = torch.take(batch['item'], ind).cpu().numpy().tolist()\n",
    "        actual = batch['item'][0].item()\n",
    "        hits.append(hit_metric(predictions, actual))\n",
    "        dcgs.append(dcg_metric(predictions, actual))\n",
    "        \n",
    "    return np.mean(hits), np.mean(dcgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "\n",
    "state_repr = State_Repr_Module(user_num, item_num, params['embedding_dim'], params['hidden_dim'])\n",
    "policy_net = Actor_DRR(params['embedding_dim'], params['hidden_dim'])\n",
    "value_net  = Critic_DRR(params['embedding_dim'] * 3, params['embedding_dim'], params['hidden_dim'])\n",
    "replay_buffer = Prioritized_Buffer(params['buffer_size'])\n",
    "\n",
    "target_value_net  = Critic_DRR(params['embedding_dim'] * 3, params['embedding_dim'], params['hidden_dim'])\n",
    "target_policy_net = Actor_DRR(params['embedding_dim'], params['hidden_dim'])\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "value_criterion  = nn.MSELoss()\n",
    "value_optimizer  = Adam(value_net.parameters(),  lr=params['value_lr'],\n",
    "                          weight_decay=params['value_decay'])\n",
    "policy_optimizer = Adam(policy_net.parameters(), lr=params['policy_lr'],\n",
    "                          weight_decay=params['policy_decay'])\n",
    "state_repr_optimizer = Adam(state_repr.parameters(), lr=params['state_repr_lr'],\n",
    "                              weight_decay=params['state_repr_decay'])\n",
    "\n",
    "writer = SummaryWriter(log_dir=params['log_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def ddpg_update(training_env, \n",
    "                step=0,\n",
    "                batch_size=params['batch_size'], \n",
    "                gamma=params['gamma'],\n",
    "                min_value=params['min_value'],\n",
    "                max_value=params['max_value'],\n",
    "                soft_tau=params['soft_tau'],\n",
    "               ):\n",
    "    beta = get_beta(step)\n",
    "    user, memory, action, reward, next_user, next_memory, done = replay_buffer.sample(batch_size, beta)\n",
    "    user        = torch.FloatTensor(user)\n",
    "    memory      = torch.FloatTensor(memory)\n",
    "    action      = torch.FloatTensor(action)\n",
    "    reward      = torch.FloatTensor(reward)\n",
    "    next_user   = torch.FloatTensor(next_user)\n",
    "    next_memory = torch.FloatTensor(next_memory)\n",
    "    done = torch.FloatTensor(done)\n",
    "    \n",
    "    state       = state_repr(user, memory)\n",
    "    policy_loss = value_net(state, policy_net(state))\n",
    "    policy_loss = -policy_loss.mean()\n",
    "    \n",
    "    next_state     = state_repr(next_user, next_memory)\n",
    "    next_action    = target_policy_net(next_state)\n",
    "    target_value   = target_value_net(next_state, next_action.detach())\n",
    "    expected_value = reward + (1.0 - done) * gamma * target_value\n",
    "    expected_value = torch.clamp(expected_value, min_value, max_value)\n",
    "\n",
    "    value = value_net(state, action)\n",
    "    value_loss = value_criterion(value, expected_value.detach())\n",
    "    \n",
    "    state_repr_optimizer.zero_grad()\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward(retain_graph=True)\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward(retain_graph=True)\n",
    "    value_optimizer.step()\n",
    "    state_repr_optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "                )\n",
    "\n",
    "    for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n",
    "\n",
    "    writer.add_histogram('value', value, step)\n",
    "    writer.add_histogram('target_value', target_value, step)\n",
    "    writer.add_histogram('expected_value', expected_value, step)\n",
    "    writer.add_histogram('policy_loss', policy_loss, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/var/folders/rp/52dczj196xsgxn2_m6kh58_80000gn/T/ipykernel_14950/3553351555.py:30: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [action]\n",
      "100%|██████████| 10/10 [00:32<00:00,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(16)\n",
    "train_env = Env(train_matrix)\n",
    "hits, dcgs = [], []\n",
    "hits_all, dcgs_all = [], []\n",
    "step, best_step = 0, 0\n",
    "step, best_step, best_step_all = 0, 0, 0\n",
    "users = np.random.permutation(appropriate_users)[:10]\n",
    "ou_noise = OUNoise(params['embedding_dim'], decay_period=10)\n",
    "\n",
    "eval_freq = 100\n",
    "all_eval_freq = 10000\n",
    "\n",
    "for u in tqdm.tqdm(users):\n",
    "    user, memory = train_env.reset(u)\n",
    "    if params['ou_noise']:\n",
    "        ou_noise.reset()\n",
    "    for t in range(int(train_matrix[u].sum())):\n",
    "        action_emb = policy_net(state_repr(user, memory))\n",
    "        if params['ou_noise']:\n",
    "            action_emb = ou_noise.get_action(action_emb.detach().cpu().numpy()[0], t)\n",
    "        action = policy_net.get_action(\n",
    "            user, \n",
    "            torch.tensor(train_env.memory[to_np(user).astype(int), :]), \n",
    "            state_repr, \n",
    "            action_emb,\n",
    "            torch.tensor(\n",
    "                [item for item in train_env.available_items \n",
    "                if item not in train_env.viewed_items]\n",
    "            ).long()\n",
    "        )\n",
    "        user, memory, reward, done = train_env.step(\n",
    "            action, \n",
    "            action_emb,\n",
    "            buffer=replay_buffer\n",
    "        )\n",
    "\n",
    "        if len(replay_buffer) > params['batch_size']:\n",
    "            ddpg_update(train_env, step=step)\n",
    "\n",
    "        if step % eval_freq == 0 and step > 0:\n",
    "            hit, dcg = run_evaluation(policy_net, state_repr, train_env.memory)\n",
    "            writer.add_scalar('hit', hit, step)\n",
    "            writer.add_scalar('dcg', dcg, step)\n",
    "            hits.append(hit)\n",
    "            dcgs.append(dcg)\n",
    "            if np.mean(np.array([hit, dcg]) - np.array([hits[best_step], dcgs[best_step]])) > 0:\n",
    "                best_step = step // eval_freq\n",
    "                torch.save(policy_net.state_dict(), params['log_dir'] + 'policy_net.pth')\n",
    "                torch.save(value_net.state_dict(), params['log_dir'] + 'value_net.pth')\n",
    "                torch.save(state_repr.state_dict(), params['log_dir'] + 'state_repr.pth')\n",
    "        if step % all_eval_freq == 0 and step > 0:\n",
    "            hit, dcg = run_evaluation(policy_net, state_repr, train_env.memory, full_loader)\n",
    "            writer.add_scalar('hit_all', hit, step)\n",
    "            writer.add_scalar('dcg_all', dcg, step)\n",
    "            hits_all.append(hit)\n",
    "            dcgs_all.append(dcg)\n",
    "            if np.mean(np.array([hit, dcg]) - np.array([hits_all[best_step_all], dcgs_all[best_step_all]])) > 0:\n",
    "                best_step_all = step // all_eval_freq\n",
    "                torch.save(policy_net.state_dict(), params['log_dir'] + 'best_policy_net.pth')\n",
    "                torch.save(value_net.state_dict(), params['log_dir'] + 'best_value_net.pth')\n",
    "                torch.save(state_repr.state_dict(), params['log_dir'] + 'best_state_repr.pth')\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), params['log_dir'] + 'policy_net_final.pth')\n",
    "torch.save(value_net.state_dict(), params['log_dir'] + 'value_net_final.pth')\n",
    "torch.save(state_repr.state_dict(), params['log_dir'] + 'state_repr_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need memory for validation, so it's better to save it and not wait next time \n",
    "with open('logs/memory.pickle', 'wb') as f:\n",
    "    pickle.dump(train_env.memory, f)\n",
    "    \n",
    "with open('logs/memory.pickle', 'rb') as f:\n",
    "    memory = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights and logs are stored in [this folder](https://drive.google.com/drive/folders/1oz6w4HBEVspvReBQ55zeIE4qw5OPD84U?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit rate:  0.45371301632835115 dcg:  0.25425667195713686\n"
     ]
    }
   ],
   "source": [
    "no_ou_state_repr = State_Repr_Module(user_num, item_num, params['embedding_dim'], params['hidden_dim'])\n",
    "no_ou_policy_net = Actor_DRR(params['embedding_dim'], params['hidden_dim'])\n",
    "no_ou_state_repr.load_state_dict(torch.load('logs/no_ou/' + 'best_state_repr.pth'))\n",
    "no_ou_policy_net.load_state_dict(torch.load('logs/no_ou/' + 'best_policy_net.pth'))\n",
    "\n",
    "hit, dcg = run_evaluation(no_ou_policy_net, no_ou_state_repr, memory, full_loader)\n",
    "print('hit rate: ', hit, 'dcg: ', dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit rate:  0.5024706798086426 dcg:  0.28013841397710176\n"
     ]
    }
   ],
   "source": [
    "ou_state_repr = State_Repr_Module(user_num, item_num, params['embedding_dim'], params['hidden_dim'])\n",
    "ou_policy_net = Actor_DRR(params['embedding_dim'], params['hidden_dim'])\n",
    "ou_state_repr.load_state_dict(torch.load('logs/ou_noise_04/' + 'best_state_repr.pth'))\n",
    "ou_policy_net.load_state_dict(torch.load('logs/ou_noise_04/' + 'best_policy_net.pth'))\n",
    "\n",
    "hit, dcg = run_evaluation(ou_policy_net, ou_state_repr, memory, full_loader)\n",
    "print('hit rate: ', hit, 'dcg: ', dcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of trained agents behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose random user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5537\n"
     ]
    }
   ],
   "source": [
    "random_user = np.random.randint(user_num)\n",
    "print(random_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        id                                    name                   genre\n26      27                     Now and Then (1995)                   Drama\n54      55                          Georgia (1995)                   Drama\n77      78              Crossing Guard, The (1995)                   Drama\n133    135                   Down Periscope (1996)                  Comedy\n199    201                     Three Wishes (1995)                   Drama\n209    211            Browning Version, The (1994)                   Drama\n607    611            Hellraiser: Bloodline (1996)    Action|Horror|Sci-Fi\n640    645          Nelly & Monsieur Arnaud (1995)                   Drama\n664    670  World of Apu, The (Apur Sansar) (1959)                   Drama\n748    758               Jar, The (Khomreh) (1992)                   Drama\n822    833                 High School High (1996)                  Comedy\n916    928                          Rebecca (1940)        Romance|Thriller\n953    965                    39 Steps, The (1935)                Thriller\n1100  1116  Single Girl, A (La Fille Seule) (1995)                   Drama\n1264  1284                   Big Sleep, The (1946)       Film-Noir|Mystery\n1436  1463                 That Old Feeling (1997)          Comedy|Romance\n1472  1504                      Hollow Reed (1996)                   Drama\n1484  1519                   Broken English (1996)                   Drama\n1485  1520                     Commandments (1997)                 Romance\n1498  1535        Love! Valour! Compassion! (1997)           Drama|Romance\n1522  1562                   Batman & Robin (1997)  Action|Adventure|Crime\n1548  1589                         Cop Land (1997)     Crime|Drama|Mystery\n1700  1753                       Half Baked (1998)                  Comedy",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>genre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>26</th>\n      <td>27</td>\n      <td>Now and Then (1995)</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>55</td>\n      <td>Georgia (1995)</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>78</td>\n      <td>Crossing Guard, The (1995)</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>135</td>\n      <td>Down Periscope (1996)</td>\n      <td>Comedy</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>201</td>\n      <td>Three Wishes (1995)</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>209</th>\n      <td>211</td>\n      <td>Browning Version, The (1994)</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>607</th>\n      <td>611</td>\n      <td>Hellraiser: Bloodline (1996)</td>\n      <td>Action|Horror|Sci-Fi</td>\n    </tr>\n    <tr>\n      <th>640</th>\n      <td>645</td>\n      <td>Nelly &amp; Monsieur Arnaud (1995)</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>664</th>\n      <td>670</td>\n      <td>World of Apu, The (Apur Sansar) (1959)</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>748</th>\n      <td>758</td>\n      <td>Jar, The (Khomreh) (1992)</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>822</th>\n      <td>833</td>\n      <td>High School High (1996)</td>\n      <td>Comedy</td>\n    </tr>\n    <tr>\n      <th>916</th>\n      <td>928</td>\n      <td>Rebecca (1940)</td>\n      <td>Romance|Thriller</td>\n    </tr>\n    <tr>\n      <th>953</th>\n      <td>965</td>\n      <td>39 Steps, The (1935)</td>\n      <td>Thriller</td>\n    </tr>\n    <tr>\n      <th>1100</th>\n      <td>1116</td>\n      <td>Single Girl, A (La Fille Seule) (1995)</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>1264</th>\n      <td>1284</td>\n      <td>Big Sleep, The (1946)</td>\n      <td>Film-Noir|Mystery</td>\n    </tr>\n    <tr>\n      <th>1436</th>\n      <td>1463</td>\n      <td>That Old Feeling (1997)</td>\n      <td>Comedy|Romance</td>\n    </tr>\n    <tr>\n      <th>1472</th>\n      <td>1504</td>\n      <td>Hollow Reed (1996)</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>1484</th>\n      <td>1519</td>\n      <td>Broken English (1996)</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>1485</th>\n      <td>1520</td>\n      <td>Commandments (1997)</td>\n      <td>Romance</td>\n    </tr>\n    <tr>\n      <th>1498</th>\n      <td>1535</td>\n      <td>Love! Valour! Compassion! (1997)</td>\n      <td>Drama|Romance</td>\n    </tr>\n    <tr>\n      <th>1522</th>\n      <td>1562</td>\n      <td>Batman &amp; Robin (1997)</td>\n      <td>Action|Adventure|Crime</td>\n    </tr>\n    <tr>\n      <th>1548</th>\n      <td>1589</td>\n      <td>Cop Land (1997)</td>\n      <td>Crime|Drama|Mystery</td>\n    </tr>\n    <tr>\n      <th>1700</th>\n      <td>1753</td>\n      <td>Half Baked (1998)</td>\n      <td>Comedy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv('data/movies.dat', sep='::', header=None, engine='python', names=['id', 'name', 'genre'], encoding=\"\")\n",
    "# in the code numeration starts with 0\n",
    "movies[movies['id'].isin(np.argwhere(test_matrix[random_user] > 0)[:, 1] + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we can recommend \"Nixon\" and \"Love Serenade\" and see next 3 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ou_policy_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [72], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m predictions \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model, state_representation \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m([\u001B[43mou_policy_net\u001B[49m, no_ou_policy_net], [ou_state_repr, no_ou_state_repr]):\n\u001B[1;32m      4\u001B[0m     example_env \u001B[38;5;241m=\u001B[39m Env(test_matrix)\n\u001B[1;32m      5\u001B[0m     user, memory \u001B[38;5;241m=\u001B[39m example_env\u001B[38;5;241m.\u001B[39mreset(random_user)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ou_policy_net' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for model, state_representation in zip([ou_policy_net, no_ou_policy_net], [ou_state_repr, no_ou_state_repr]):\n",
    "    example_env = Env(test_matrix)\n",
    "    user, memory = example_env.reset(random_user)\n",
    "\n",
    "    user, memory, reward, _ = example_env.step(torch.tensor([13]))\n",
    "    user, memory, reward, _ = example_env.step(torch.tensor([1584]))\n",
    "    preds = []\n",
    "    for _ in range(3):\n",
    "        action_emb = model(state_representation(user, memory))\n",
    "        action = model.get_action(\n",
    "            user, \n",
    "            torch.tensor(example_env.memory[to_np(user).astype(int), :]), \n",
    "            state_representation, \n",
    "            action_emb,\n",
    "            torch.tensor(\n",
    "                [item for item in example_env.available_items \n",
    "                if item not in example_env.viewed_items]\n",
    "            ).long()\n",
    "        )\n",
    "        user, memory, reward, _ = example_env.step(action)\n",
    "        preds.append(action)\n",
    "\n",
    "    predictions.append(preds)\n",
    "\n",
    "print(predictions[0])\n",
    "print(predictions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained with OU noise recommended related `Comedy` and `Documentary` after that switch recommendations to nonrelated `Crime|Film-Noir|Thriller`.\n",
    "Model trained without OU noise recommended `Comedy|Drama`, `Children's|Comedy`, `Drama` (two of them are related).\n",
    "\n",
    "Both models seems to be reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=img/learning_curve.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logs are consistent with expectations. Adding noise increase metrics (std=0.4 performs the best, after 0.6 model starts to degrade)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "s-iWfb5TixZc",
    "baIIDXdAixaH"
   ],
   "name": "pytorch.pipelines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
